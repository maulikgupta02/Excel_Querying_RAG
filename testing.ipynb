{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# del os.environ[\"API_VERSION_4o\"]\n",
    "load_dotenv()\n",
    "\n",
    "# print(os.getenv(\"AZURE_OPENAI_4o_API_KEY\"))\n",
    "# print(os.getenv(\"API_VERSION_4o\"))\n",
    "# print(os.getenv(\"AZURE_OPENAI_4o_ENDPOINT\"))\n",
    "\n",
    "def generate_response(query):\n",
    "    schema_info=\"\"\n",
    "    with open(\"temp/data_description.txt\",\"r\") as file:\n",
    "        for reader in file.readlines():\n",
    "            schema_info+=reader\n",
    "\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_4o_API_KEY\"),  \n",
    "    api_version = os.getenv(\"API_VERSION_4o\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_4o_ENDPOINT\")\n",
    "    )\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", # model = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"You have access to a pandas dataframe named df. \\\n",
    "    Here is a name and decription of each column in the dataframe\n",
    "\n",
    "    {schema_info}\n",
    "\n",
    "    Don't asssume any other column and feature name, it should be exactly the same. \\\n",
    "    Given a user question about the dataframe,Provide only the Python code to answer it.\\\n",
    "    Don't assume you have access to any libraries other than built-in Python ones and pandas only. \\\n",
    "    You don't have any other data source other than dataframe df. \\\n",
    "    Make sure to refer only to the variables mentioned above. \\\n",
    "    Give the script to wrap it inside the custom function name generated_response(). \\\n",
    "    Return a single, new dataframe with only relevant and suitable column names according to the user query without modifying df \\\n",
    "    Give only required columns. \\\n",
    "    Do not add any print statements. \\\n",
    "    Do not alter placeholder values . \\\n",
    "    DO NOT REMOVE ANGULAR BRACKETS\"\"\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": f\"{query}\"},\n",
    "\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # print(response.choices[0].message.content)\n",
    "\n",
    "    # script=\"\"\n",
    "    # for line in response.choices[0].message.content.split(\"```\")[1].split(\"\\n\")[1:]:\n",
    "    #     script+=line+\"\\n\"\n",
    "\n",
    "    script=response.choices[0].message.content\n",
    "    temp=script.split(\"```\")[1]\n",
    "    script=temp[6:]\n",
    "\n",
    "    return script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm_advanced module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def generate_response_from_filtered_data(dataframe,query):\n",
    "\n",
    "    schema_info=\"\"\n",
    "    with open(\"temp/data_description.txt\",\"r\") as file:\n",
    "        for reader in file.readlines():\n",
    "            schema_info+=reader\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_4o_API_KEY\"),  \n",
    "    api_version = os.getenv(\"API_VERSION_4o\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_4o_ENDPOINT\")\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"\n",
    "             You are responsible for explaining the answer to the user based on the dataframe, schema and the query provided to you.\\\n",
    "    The following dataframe is already filtered according to user query and columns renamed: {dataframe.to_csv(index=False)} \\\n",
    "    Assume it contains all the information asked in query. \\\n",
    "    This is the description of original dataset: {schema_info} \\\n",
    "    If query cannot be answered from the provided data, simply explain the reason for the same.\\\n",
    "    Do not give any tabular data. \\\n",
    "    Do not include any reasonings and warnings, directly respond to the query. \\\n",
    "    Do not give any warnings. \\\n",
    "    Do not remove angular <> brackets, keep those values as it is. \\\n",
    "    Do not alter placeholder values . \\\n",
    "    Empty dataframe means there were no such records.\"\"\"} ,\n",
    "    \n",
    "\n",
    "    {\"role\": \"user\", \"content\": f\"{query}\"},\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    RESPONSE=response.choices[0].message.content\n",
    "\n",
    "    return RESPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anonymizer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "# from presidio_analyzer import PatternRecognizer, Pattern\n",
    "# import pandas as pd\n",
    "\n",
    "# # For dataframe only\n",
    "# columns_to_anonymize = [\"customer name\",\"IC01 Code\",\"sysname\",\"TMKey\",\"sdwan_ip_admin\"] # list all the column names to anonymize\n",
    "# columns_to_anonymize_deny_list = [\"customer name\"] # list all the column names to anonymize via deny list\n",
    "\n",
    "# # Read data as dataset for using anonimization via deny list\n",
    "# dataset = pd.read_excel(\"Power BI data-Security.xlsx\",skiprows=2)\n",
    "\n",
    "# anonymizer = PresidioReversibleAnonymizer(\n",
    "#     analyzed_fields=[], # choose inbuilt fields to be considered for anonimyzation\n",
    "#     faker_seed=42,\n",
    "# )\n",
    "\n",
    "# # Define custom regex patterns to add in presidio for anonimyzation --> anonymized entries will be named 'support_entity_{occurence_no}'\n",
    "# SYSTEM_NAME_pattern = Pattern(name=\"sysname_pattern\", regex=\"[a-z]{4}[0-9]{3,4}\", score=0.8)\n",
    "# IC01_pattern = Pattern(name=\"ic01_pattern\", regex=\"[0-9]{4,7}\",score=1.0)\n",
    "# IPV4_IP_pattern = Pattern(\"customer_name_alnum_pattern\",regex=\"[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}\", score=0.8)\n",
    "\n",
    "# anonymizer.add_recognizer(PatternRecognizer(supported_entity=\"SYSTEM_NAME\", patterns=[SYSTEM_NAME_pattern]))\n",
    "# anonymizer.add_recognizer(PatternRecognizer(supported_entity=\"IC01\", patterns=[IC01_pattern]))\n",
    "# anonymizer.add_recognizer(PatternRecognizer(supported_entity=\"sdwan_ip_admin\", patterns=[IPV4_IP_pattern]))\n",
    "\n",
    "# for column in columns_to_anonymize_deny_list:\n",
    "#     anonymizer.add_recognizer(PatternRecognizer(supported_entity=f\"{column}\",\n",
    "#                                                 deny_list=list(dataset[column].unique()),\n",
    "#                                                 deny_list_score=0.5)) \n",
    "\n",
    "\n",
    "# def anonymize_text(text):\n",
    "#     return anonymizer.anonymize(text)\n",
    "\n",
    "\n",
    "# def anonymize_dataframe(df):\n",
    "#     for column in df.columns:\n",
    "#         if column in columns_to_anonymize:\n",
    "#             df[column] = df[column].astype(str)\n",
    "#             df[column] = df[column].apply(anonymizer.anonymize)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def deanonimize_text(text):\n",
    "#     return anonymizer.deanonymize(text)\n",
    "\n",
    "\n",
    "# def deanonymize_dataframe(df):\n",
    "#     for column in df.columns:\n",
    "#         if column in columns_to_anonymize:\n",
    "#             df[column] = df[column].astype(str)\n",
    "#             df[column] = df[column].apply(anonymizer.deanonymize)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# df=pd.read_excel(\"Power BI data-Security.xlsx\",skiprows=2)\n",
    "# df_anonymized=anonymize_dataframe(df)\n",
    "# df_anonymized.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_df_1=df_anonymized.copy()\n",
    "# for column in df_anonymized.columns:\n",
    "#     if column in columns_to_anonymize:\n",
    "#         actual_df_1[column] = actual_df_1[column].astype(str)\n",
    "#         actual_df_1[column] = actual_df_1[column].apply(anonymizer.deanonymize)\n",
    "  \n",
    "# actual_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anonymizer.save_deanonymizer_mapping(\"anonymization_object.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_anonymized.to_csv(\"anonymized_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "\n",
    "# # Initialize the AnonymizerEngine\n",
    "# new_anonymizer =  PresidioReversibleAnonymizer(\n",
    "#     analyzed_fields=[], # choose inbuilt fields to be considered for anonimyzation\n",
    "#     faker_seed=42,\n",
    "# )\n",
    "\n",
    "# columns_to_anonymize = [\"customer name\",\"IC01 Code\",\"sysname\",\"TMKey\",\"sdwan_ip_admin\"] # list all the column names to anonymize\n",
    "\n",
    "# new_anonymizer.load_deanonymizer_mapping(\"anonymization_object.json\")\n",
    "\n",
    "# def deanonymize_dataframe(df):\n",
    "#     for column in df.columns:\n",
    "#         if column in columns_to_anonymize:\n",
    "#             df[column] = df[column].astype(str)\n",
    "#             df[column] = df[column].apply(anonymizer.deanonymize)\n",
    "\n",
    "#     return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_df_2=df_anonymized.head(10)\n",
    "# for column in df_anonymized.columns:\n",
    "#     if column in columns_to_anonymize:\n",
    "#         actual_df_2[column] = actual_df_2[column].astype(str)\n",
    "#         actual_df_2[column] = actual_df_2[column].apply(new_anonymizer.deanonymize)\n",
    "\n",
    "# actual_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add table columns data to the schema desciption file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# feature_columns=[\"type_service\",\"Service\",\"TM Manager\",\"country\",\"region\",\"Sales Country\",\"Sales Region\"]\n",
    "\n",
    "# df = pd.read_csv(\"anonymized_data.csv\")\n",
    "\n",
    "# with open(\"desc.txt\", 'r') as f_source:\n",
    "#     file_content = f_source.read()\n",
    "\n",
    "# with open(\"InfoChest.txt\", \"w\") as file:\n",
    "#     file.write(file_content)\n",
    "#     file.write(\"\\n\\nUnique Column Values in the dataframe:\\n\\n\")\n",
    "\n",
    "#     for column in df.columns:\n",
    "#         if column not in feature_columns:\n",
    "#             continue\n",
    "#         file.write(f\"Column name: {column}\\nUnique Values:\\n\")\n",
    "#         unique_values = df[column].unique()\n",
    "#         file.writelines(f\"{value}\\n\" for value in unique_values)\n",
    "#         file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_excel(\"Power BI data-Security.xlsx\",skiprows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"Give all the device from the sales country uae\"\n",
    "# query=new_anonymizer.anonymize(query)\n",
    "# error=\"\"\n",
    "# counter=0\n",
    "# while True:\n",
    "#     try:\n",
    "#         prompt=generate_response(\"Provide only the Python code wrapped inside the function generated_response() with no args to \"+query+error)\n",
    "#         print(\"The generated script is:-\")\n",
    "#         print(prompt)\n",
    "#         exec(prompt)\n",
    "#         RESPONSE=generated_response()\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         error=f\"\"\"got error as {e} on your previous response {prompt}, give correct code\"\"\"\n",
    "#         pass\n",
    "#     counter+=1\n",
    "#     if counter>=5:\n",
    "#         RESPONSE=\"No related information found in the dataframe\"\n",
    "#         break\n",
    "\n",
    "# print()\n",
    "# print(\"*\"*100)\n",
    "# print(\"response from the 1st llm is:- \")\n",
    "# print(\"*\"*100)\n",
    "# print(RESPONSE)\n",
    "\n",
    "# # df_anonymized=anonymize_dataframe(RESPONSE)\n",
    "# # print()\n",
    "# # print(\"*\"*100)\n",
    "# # print(\"The anonymized df is:-\")\n",
    "# # print(df_anonymized)\n",
    "\n",
    "# # anonymized_query=anonymize_text(query)\n",
    "# # anonymized_RESPONSE2=generate_response_from_filtered_data(df_anonymized,anonymized_query)\n",
    "# # RESPONSE2=deanonimize_text(anonymized_RESPONSE2)\n",
    "\n",
    "# # print()\n",
    "# # print(\"*\"*100)\n",
    "# # print(\"response from the 2nd llm is:- \")\n",
    "# # print(RESPONSE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"Give all the sdwan that are vul\"\n",
    "# error=\"\"\n",
    "# counter=0\n",
    "# while True:\n",
    "#     try:\n",
    "#         prompt=generate_response(\"Provide only the Python code wrapped inside the function generated_response() to \"+query+error)\n",
    "#         print(prompt)\n",
    "#         exec(prompt)\n",
    "#         RESPONSE=generated_response()\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         error=f\"\"\"got error as {e} on your previous response {prompt}.\"\"\"\n",
    "#         pass\n",
    "#     counter+=1\n",
    "#     if counter>=5:\n",
    "#         RESPONSE=\"No related information found in the dataframe\"\n",
    "#         break\n",
    "\n",
    "# print()\n",
    "# print(\"*\"*100)\n",
    "# print(\"resonse from the 1st llm is:- \")\n",
    "# print(RESPONSE)\n",
    "\n",
    "# # df_anonymized=anonymize_dataframe(RESPONSE)\n",
    "# # print()\n",
    "# # print(\"*\"*100)\n",
    "# # print(\"The anonymized df is:-\")\n",
    "# # print(df_anonymized)\n",
    "\n",
    "# # anonymized_query=anonymize_text(query)\n",
    "# # anonymized_RESPONSE2=generate_response_from_filtered_data(df_anonymized,anonymized_query)\n",
    "# # RESPONSE2=deanonimize_text(anonymized_RESPONSE2)\n",
    "\n",
    "# # print()\n",
    "# # print(\"*\"*100)\n",
    "# # print(\"response from the 2nd llm is:- \")\n",
    "# # print(RESPONSE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"Give all the devices with ic01 – {690680}.\"\n",
    "# error=\"\"\n",
    "# counter=0\n",
    "# while True:\n",
    "#     try:\n",
    "#         prompt=generate_response(\"Provide only the Python code wrapped inside the function generated_response() to \"+query+error)\n",
    "#         print(prompt)\n",
    "#         exec(prompt)\n",
    "#         RESPONSE=generated_response()\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         error=f\"\"\"got error as {e} on your previous response {prompt}.\"\"\"\n",
    "#         pass\n",
    "#     counter+=1\n",
    "#     if counter>=5:\n",
    "#         RESPONSE=\"No related information found in the dataframe\"\n",
    "#         break\n",
    "\n",
    "# print()\n",
    "# print(\"*\"*100)\n",
    "# print(\"resonse from the 1st llm is:- \")\n",
    "# print(RESPONSE)\n",
    "\n",
    "# # df_anonymized=anonymize_dataframe(RESPONSE)\n",
    "# # print()\n",
    "# # print(\"*\"*20)\n",
    "# # print(\"The anonymized df is:-\")\n",
    "# # print(df_anonymized)\n",
    "\n",
    "# # anonymized_query=anonymize_text(query)\n",
    "# # anonymized_RESPONSE2=generate_response_from_filtered_data(df_anonymized,anonymized_query)\n",
    "# # RESPONSE2=deanonimize_text(anonymized_RESPONSE2)\n",
    "\n",
    "# # print()\n",
    "# # print(\"*\"*20)\n",
    "# # print(\"response from the 2nd llm is:- \")\n",
    "# # print(RESPONSE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM to restructure query to match the dataframe features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# del os.environ[\"API_VERSION_4o\"]\n",
    "load_dotenv()\n",
    "\n",
    "# print(os.getenv(\"AZURE_OPENAI_4o_API_KEY\"))\n",
    "# print(os.getenv(\"API_VERSION_4o\"))\n",
    "# print(os.getenv(\"AZURE_OPENAI_4o_ENDPOINT\"))\n",
    "\n",
    "def generate_query(query):\n",
    "    dfvalues=\"\"\n",
    "    with open(\"temp/InfoChest.txt\",\"r\") as file:\n",
    "        for reader in file.readlines():\n",
    "            dfvalues+=reader\n",
    "\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_4o_API_KEY\"),  \n",
    "    api_version = os.getenv(\"API_VERSION_4o\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_4o_ENDPOINT\")\n",
    "    )\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", # model = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"You have a dataframe information as an input. \\\n",
    "        It contains column namea and the unique values of all the columns. \\\n",
    "\n",
    "    {dfvalues}\n",
    "\n",
    "    Don't asssume any other column and feature name, it should be exactly the same. \\\n",
    "    Given a user query, replace the given column names and the column values with the actual ones from that column, make sure to not remove anything. \\\n",
    "    Do not change or replace values within <> . \\\n",
    "    Do not alter placeholder values . \\\n",
    "    Restructure the query for better understanding of columns and its values. \\\n",
    "    Just give the new query directly\"\"\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": f\"{query}\"},\n",
    "\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # print(response.choices[0].message.content)\n",
    "\n",
    "    # script=\"\"\n",
    "    # for line in response.choices[0].message.content.split(\"```\")[1].split(\"\\n\")[1:]:\n",
    "    #     script+=line+\"\\n\"\n",
    "\n",
    "    script=response.choices[0].message.content\n",
    "    # temp=script.split(\"```\")[1]\n",
    "    # script=temp[6:]\n",
    "\n",
    "    return script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53997 entries, 0 to 53996\n",
      "Data columns (total 56 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   customer name             53997 non-null  object\n",
      " 1   IC01 Code                 53997 non-null  object\n",
      " 2   sysname                   53997 non-null  object\n",
      " 3   #clean device             53997 non-null  bool  \n",
      " 4   #PSIRT Vln                53997 non-null  bool  \n",
      " 5   #SDWAN vln                53997 non-null  bool  \n",
      " 6   YOGI_COG issues           53997 non-null  bool  \n",
      " 7   not SSHv2                 53997 non-null  bool  \n",
      " 8   not SNMPv3                53997 non-null  bool  \n",
      " 9   EOL                       53997 non-null  bool  \n",
      " 10  Non Standard              53997 non-null  bool  \n",
      " 11  country                   53997 non-null  object\n",
      " 12  region                    53997 non-null  object\n",
      " 13  chassis_api               53997 non-null  object\n",
      " 14  ios_version               53997 non-null  object\n",
      " 15  type_service              53997 non-null  object\n",
      " 16  etat_sig_routeur          53997 non-null  object\n",
      " 17  pooling_status            53997 non-null  object\n",
      " 18  ip_admin                  53997 non-null  object\n",
      " 19  type_routeur              53997 non-null  object\n",
      " 20  constructeur              53997 non-null  object\n",
      " 21  Sales Country             53997 non-null  object\n",
      " 22  Sales Region              53997 non-null  object\n",
      " 23  Service                   53997 non-null  object\n",
      " 24  psirt_chassis_api         53997 non-null  object\n",
      " 25  psirt_ios_version         53997 non-null  object\n",
      " 26  psirt_target_ios_version  53997 non-null  object\n",
      " 27  yogi_psirt_sa_id          53997 non-null  object\n",
      " 28  psirt_type_router         53997 non-null  object\n",
      " 29  sdwan_chassis_api         53997 non-null  object\n",
      " 30  sdwan_ios_version         53997 non-null  object\n",
      " 31  sdwan_target_ios_version  53997 non-null  object\n",
      " 32  sdwan_ip_admin            53997 non-null  object\n",
      " 33  sdwan_router_role         53997 non-null  object\n",
      " 34  sdwan_type_router         53997 non-null  object\n",
      " 35  sdwan_constructeur        53997 non-null  object\n",
      " 36  cog_chassis_api           53997 non-null  object\n",
      " 37  cog_ios_version           53997 non-null  object\n",
      " 38  yogi_cog_code_verif       53997 non-null  object\n",
      " 39  ssh_chassis_api           53997 non-null  object\n",
      " 40  ssh_ios_version           53997 non-null  object\n",
      " 41  snmp_chassis_api          53997 non-null  object\n",
      " 42  snmp_ios_version          53997 non-null  object\n",
      " 43  snmp_version              53997 non-null  object\n",
      " 44  eol_ios_version           53997 non-null  object\n",
      " 45  eol_scope                 53997 non-null  object\n",
      " 46  eol_descope_reason        53997 non-null  object\n",
      " 47  eol_status                53997 non-null  object\n",
      " 48  eol_existing_chassis      53997 non-null  object\n",
      " 49  eol_new_chassis           53997 non-null  object\n",
      " 50  non_standard_detail       53997 non-null  object\n",
      " 51  Entry Date                53997 non-null  object\n",
      " 52  TMKey                     53997 non-null  object\n",
      " 53  TM Manager                53997 non-null  object\n",
      " 54  router                    53997 non-null  bool  \n",
      " 55  switch                    53997 non-null  bool  \n",
      "dtypes: bool(10), object(46)\n",
      "memory usage: 19.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_parquet(\"processed_data/raw_data.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "\n",
    "# # Initialize the AnonymizerEngine\n",
    "# new_anonymizer =  PresidioReversibleAnonymizer(\n",
    "#     analyzed_fields=[], # choose inbuilt fields to be considered for anonimyzation\n",
    "#     faker_seed=42,\n",
    "# )\n",
    "\n",
    "# columns_to_anonymize = [\"customer name\",\"IC01 Code\",\"sysname\",\"TMKey\",\"sdwan_ip_admin\"]\n",
    "\n",
    "# new_anonymizer.load_deanonymizer_mapping(\"anonymization_object.json\")\n",
    "\n",
    "# new_anonymizer.analyzed_fields=[\"EMAIL\"]\n",
    "\n",
    "# def anonymize_text(text):\n",
    "#     return new_anonymizer.anonymize(text)\n",
    "\n",
    "\n",
    "# def anonymize_dataframe(df):\n",
    "#     for column in df.columns:\n",
    "#         if column in columns_to_anonymize:\n",
    "#             df[column] = df[column].astype(str)\n",
    "#             df[column] = df[column].apply(new_anonymizer.anonymize)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def deanonymize_text(text):\n",
    "#     return new_anonymizer.deanonymize(text)\n",
    "\n",
    "\n",
    "# def deanonymize_dataframe(df):\n",
    "#     for column in df.columns:\n",
    "#         if column in columns_to_anonymize:\n",
    "#             df[column] = df[column].astype(str)\n",
    "#             df[column] = df[column].apply(new_anonymizer.deanonymize)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anonymizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Original Query\n",
      "****************************************************************************************************\n",
      "Give all the devices with ic01 – {60640}, only the system name and comapny\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Anonymized Query\n",
      "****************************************************************************************************\n",
      "Give all the devices with ic01 – {<IC01 Code placeholder>}, only the system name and comapny\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Restructured Anonymized Query\n",
      "****************************************************************************************************\n",
      "Give all the devices with IC01 Code – {<IC01 Code placeholder>}, only the sysname and customer name\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Generated pandas Script (Anonymized)\n",
      "****************************************************************************************************\n",
      "\n",
      "def generated_response():\n",
      "    return df[df['IC01 Code'] == '<IC01 Code placeholder>'][['sysname', 'customer name']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Generated pandas Script (Deanonymized)\n",
      "****************************************************************************************************\n",
      "\n",
      "def generated_response():\n",
      "    return df[df['IC01 Code'] == '60640'][['sysname', 'customer name']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "response from the 1st llm is:- \n",
      "****************************************************************************************************\n",
      "     sysname   customer name\n",
      "28  pcan1091  9air co.  ltd.\n",
      "29  pcan1092  9air co.  ltd.\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "response from the 2nd llm is:- \n",
      "****************************************************************************************************\n",
      "pcan1091,9air co.  ltd.\n",
      "pcan1092,9air co.  ltd.\n",
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Respective costs for llms are:- \n",
      "****************************************************************************************************\n",
      "tokens used by the query_restructuring llm: [2238, 24] cost: 0.01155\n",
      "tokens used by the filtration llm: [1708, 30] cost: 0.008990000000000001\n",
      "tokens used by response llm: [1661, 24] cost: 0.008664999999999999\n",
      "total cost($) for the query: 0.029205\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "total_tokens=0\n",
    "tokens_llm_1=[]\n",
    "tokens_llm_2=[]\n",
    "tokens_llm_3=[]\n",
    "\n",
    "dfvalues=\"\"\n",
    "with open(\"temp/InfoChest.txt\",\"r\") as file:\n",
    "    for reader in file.readlines():\n",
    "        dfvalues+=reader\n",
    "\n",
    "schema_info=\"\"\n",
    "with open(\"temp/data_description.txt\",\"r\") as file:\n",
    "    for reader in file.readlines():\n",
    "        schema_info+=reader\n",
    "\n",
    "costpertokenin=0.005/1000\n",
    "costpertokenout=0.015/1000\n",
    "\n",
    "query=\"Give all the devices with ic01 – {60640}, only the system name and comapny\"\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"Original Query\")\n",
    "print(\"*\"*100)\n",
    "print(query)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "anonymized_query=anonymize_text(query)\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"Anonymized Query\")\n",
    "print(\"*\"*100)\n",
    "print(anonymized_query)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "tokens_llm_1.append(len(enc.encode(anonymized_query+f\"\"\"You have a dataframe information as an input. \\\n",
    "        It contains column namea and the unique values of all the columns. \\\n",
    "\n",
    "    {dfvalues}\n",
    "\n",
    "    Don't asssume any other column and feature name, it should be exactly the same. \\\n",
    "    Given a user query, replace the given column names and the column values with the actual ones from that column, make sure to not remove anything. \\\n",
    "    Do not change or replace values within <> . \\\n",
    "    Do not alter placeholder values . \\\n",
    "    Restructure the query for better understanding of columns and its values. \\\n",
    "    Just give the new query directly\"\"\")))\n",
    "restructred_query_anonymized = generate_query(anonymized_query)\n",
    "tokens_llm_1.append(len(enc.encode(restructred_query_anonymized)))\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"Restructured Anonymized Query\")\n",
    "print(\"*\"*100)\n",
    "print(restructred_query_anonymized)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"Generated pandas Script (Anonymized)\")\n",
    "print(\"*\"*100)\n",
    "\n",
    "error=\"\"\n",
    "counter=0\n",
    "while True:\n",
    "    try:\n",
    "        tokens_llm_2.append(len(enc.encode(\"Provide only the Python code wrapped inside the function generated_response() with no args to \"+restructred_query_anonymized+error+\" and generate a dataframe\"+f\"\"\"You have access to a pandas dataframe named df. \\\n",
    "    Here is a name and decription of each column in the dataframe\n",
    "\n",
    "    {schema_info}\n",
    "\n",
    "    Don't asssume any other column and feature name, it should be exactly the same. \\\n",
    "    Given a user question about the dataframe,Provide only the Python code to answer it.\\\n",
    "    Don't assume you have access to any libraries other than built-in Python ones and pandas only. \\\n",
    "    You don't have any other data source other than dataframe df. \\\n",
    "    Make sure to refer only to the variables mentioned above. \\\n",
    "    Give the script to wrap it inside the custom function name generated_response(). \\\n",
    "    Return a single, new dataframe with only relevant and suitable column names according to the user query without modifying df \\\n",
    "    Give only required columns. \\\n",
    "    Do not add any print statements. \\\n",
    "    Do not alter placeholder values . \\\n",
    "    DO NOT REMOVE ANGULAR BRACKETS\"\"\")))\n",
    "        anonymized_prompt=generate_response(\"Provide only the Python code wrapped inside the function generated_response() with no args to \"+restructred_query_anonymized+error+\" and generate a dataframe\")\n",
    "        tokens_llm_2.append(len(enc.encode(anonymized_prompt)))\n",
    "        print(anonymized_prompt)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        print(\"*\"*100)\n",
    "        print(\"Generated pandas Script (Deanonymized)\")\n",
    "        print(\"*\"*100)\n",
    "\n",
    "        prompt=deanonymize_text(anonymized_prompt)\n",
    "        print(prompt)\n",
    "\n",
    "        exec(prompt)\n",
    "        RESPONSE=generated_response()\n",
    "        # RESPONSE=deanonymize_dataframe(RESPONSE)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        error=f\"\"\"got error as {e} on your previous response {prompt}, give correct code\"\"\"\n",
    "        pass\n",
    "    counter+=1\n",
    "    if counter>=5:\n",
    "        RESPONSE=\"Could not generate response, try restructuring your query !\"\n",
    "        break\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"*\"*100)\n",
    "print(\"response from the 1st llm is:- \")\n",
    "print(\"*\"*100)\n",
    "print(RESPONSE)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"*\"*100)\n",
    "print(\"response from the 2nd llm is:- \")\n",
    "print(\"*\"*100)\n",
    "\n",
    "if RESPONSE.shape[0]*RESPONSE.shape[1] > 5000:\n",
    "    print(\"Cannot be processsed further due to large size\")\n",
    "\n",
    "else:\n",
    "    tokens_llm_3.append(len(enc.encode(f\"\"\"\n",
    "             You are responsible for explaining the answer to the user based on the dataframe, schema and the query provided to you.\\\n",
    "    The following dataframe is already filtered according to user query and columns renamed: {anonymize_dataframe(RESPONSE).to_csv(index=False)} \\\n",
    "    Assume it contains all the information asked in query. \\\n",
    "    This is the description of original dataset: {schema_info} \\\n",
    "    If query cannot be answered from the provided data, simply explain the reason for the same.\\\n",
    "    Do not give any tabular data. \\\n",
    "    Do not include any reasonings and warnings, directly respond to the query. \\\n",
    "    Do not give any warnings. \\\n",
    "    Do not remove angular <> brackets, keep those values as it is. \\\n",
    "    Do not alter placeholder values . \\\n",
    "    Empty dataframe means there were no such records.\"\"\")))\n",
    "    filtered_RESPONSE=deanonymize_text(generate_response_from_filtered_data(dataframe=anonymize_dataframe(RESPONSE),query=anonymized_query))\n",
    "    tokens_llm_3.append(len(enc.encode(filtered_RESPONSE)))\n",
    "    print(filtered_RESPONSE)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"*\"*100)\n",
    "print(\"Respective costs for llms are:- \")\n",
    "print(\"*\"*100)\n",
    "print(\"tokens used by the query_restructuring llm:\",tokens_llm_1,\"cost:\",tokens_llm_1[0]*costpertokenin+tokens_llm_1[1]*costpertokenout)\n",
    "print(\"tokens used by the filtration llm:\",tokens_llm_2,\"cost:\",tokens_llm_2[0]*costpertokenin+tokens_llm_2[1]*costpertokenout)\n",
    "print(\"tokens used by response llm:\",tokens_llm_3,\"cost:\",tokens_llm_3[0]*costpertokenin+tokens_llm_3[1]*costpertokenout)\n",
    "print(\"total cost($) for the query:\", tokens_llm_1[0]*costpertokenin+tokens_llm_1[1]*costpertokenout+tokens_llm_2[0]*costpertokenin+tokens_llm_2[1]*costpertokenout+tokens_llm_3[0]*costpertokenin+tokens_llm_3[1]*costpertokenout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
